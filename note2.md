## 1.58bit(BitNet系)次世代LLMとED学習法：適用可能性と成立条件

　または

## ED法は次世代LLM学習に足り得るか：1.58bit/低bit化との接点

　または

## Transformer以後の軽量“前頭前野”実装：ED法・低bit・疎結合の設計論

　　　　　　　　　　　　　　　　　　　　　　　　　　　　2026/01/30
　　　　　　　　　　　　　　　　　　　　　　　　　　　　X/Twitter:nyama3310
　　　　　　　　　　　　　　　　　　　　　　　　　　　　Mail:osaisenware@gmail.com

※本稿は完成した理論を提示するものではない。  
　むしろ、どこが未完成であり、  
　どこに注意を払うべきかを記録することを目的とする。  
※本稿は、2026年当時の最高のAIの一つである、ChatGPT-5.2の全面的な助力により作成された。  
　『彼』の幅広い知識、深い洞察力と論理文書構成能力、的確なコード解析能力無くして、  
　この文書はあり得なかった。特に、3,4章は、彼の作文、ほぼ、そのままである。  
　よって、彼を、１(疑似)人格と認めた上で、その貢献を称え、本稿の共同執筆者と冠したい。  
　勿論、執筆者と言っても、全ての作文指示は私から出ており、また、彼には、自己決定権はなく、  
　従って、責任主体にはなり得ないので、本稿における判断・方向付け・最終的な記述責任は、  
　すべて筆者に帰属する、という点には御留意頂きたい。

### \[はじめに:問題意識]

* Transformerの発見により、人類は、AIの発明(=脳の前頭前野的『機能』の再現)に成功したが、  
  その『実装』は、旧来の計算機上の過大な行列演算の集合体であり、脳の実装様式には、まだ遠く及ばない。
* この実装上の課題への一つの回答として、パラメータを \[1, 0, -1] の3値のみに制限する  
  1.58bit（≒log₂3）BitNet系と呼ばれる次世代LLM構成法が検討されている。
* さらに、1.58bit LLMと相性が良いと考えられる、ED法(誤差拡散法)と呼ばれる学習手法が存在する。  
  これは現在主流のTransformerに用いられているBP法(誤差逆伝播法)とは全く異なる学習則であり、  
  いまだ発展途上の段階にある。
* 本文書では、1.58bit LLMへED法を適用することの意義および課題を検討し、成立条件の整理と、  
  将来に向けた展望を行う。
* 付録として、Transformerより約20年も前に、Transformerの「別実装」とも解釈し得るED法を発案し、  
  これを Neko Fight というゲームとして実装した偉大な先人(故人)・金子勇氏の足跡に言及し、  
  その先駆的業績を称える。



### \[1章:LLM(=前頭前野)の実現方法論]

#### (1-1:現在のLLMの実装方法:Transformerの構成要素)

* 現代の大規模言語モデル(LLM)は、Transformerという理論を基に構成されており、  
  その中核的な理論は、自己注意機構(Self-attention)である。また、脳神経学によると、  
  人間の脳の前頭前野にも、自己注意機構と同じような機能があると考えられており、  
  LLMの本質とは、コンピュータ上で、巨大な行列演算を行って、  
  個々の神経細胞の動きをシミュレートして、脳の自己注意機構を再現する事だと言える。
* 但し、LLMは、自己注意機構だけで成立するものではなく、LLMを訓練する為の学習則と、  
  その学習則を支える様々な構成要素(レシピ)を必要とする。  
  現代のLLMでは、学習則として、BP(誤差逆伝搬)法が用いられ、それを支えるレシピとして、  
  ・ゲイン制御(A)：暴走も沈黙もせず、一定の制御帯域で動く/重み絶対値増大・飽和を止める  
  　→レシピ：LayerNorm / RMSNorm、weight decay、gradient clipping、Adam系の適応学習率  
  　→ない場合、脳で起きる病理:固着(反応が変わらない)、無気力(全入力に鈍感)、暴走(一部入力への過剰反応)  
  ・局在化(B)：因果の近い部分が主に更新される(クレジット割当)/誤差は拡散しても可塑性は局所  
  　→レシピ：multi-head attention、出力層での softmax + cross entropy、勾配計算の厳密な経路管理  
  　→ない場合、脳で起きる病理:てんかん様の同期発火(局所皮質に限定されるべき神経活動が全域に波及する)  
  ・表現の分化(C)：同じ入力でも別の回路が別の意味を担う  
  　→レシピ：明示的には存在しない(大規模化・ランダム性に依存)  
  　→ない場合、脳で起きる病理:無気力、ノイズ過敏、汎化不全  
  が必要である。
* これらの巨大行列と各種レシピの演算、すなわち、LLMの動作時、特に、学習時には、メモリ、CPU/GPUパワーなど、  
  計算機リソースを、大量に消費する結果となり、LLMが、高価なハードウェア/巨大なデータセンターでしか  
  稼働しない要因となっている。

#### (1-2:前頭前野での自己注意:生物学的なTransformerの相当の構成要素)

* 一方、脳の前頭前野での自己注意機構は、次のような機構を通じて実現されていると考えられる。  
  なお、以下の説明は、脳神経学の専門知識となるので、詳細は省略し、概略にとどめる。  
  ・注意：競合を“バイアス”でねじ曲げる（biased competition）  
  　神経科学では、刺激は皮質内で表象を奪い合い、そこにトップダウン信号が“偏り（bias）”をかけて勝者を作る、  
  　という枠組みが強い（Desimone \& Duncan 1995 系）。  
  　トップダウンの発信源として前頭前野が重要、という整理もこの系譜で語られる。  
  　→この動作は、“どれを見るべきか”を重みで決めるという点で、Transformerの「重み付き和」と  
  　　類似していると考えられる。  
  ・自己参照:ワーキングメモリは再帰回路の“持続活動”で維持される、という古典  
  　前頭前野のワーキングメモリは、刺激が消えた後も活動が持続する（persistent activity）という見方が古典的に強く、  
  　再帰回路（recurrent circuit）の“反響（reverberation）”で支えられるというモデルがある。  
  　（近年は「持続発火だけが全てではない」方向の更新もあるが、少なくとも“再帰が鍵”は残る。）  
  　→この動作は、過去の情報を内部で保持し、必要に応じて参照するという点で、Transformerの  
  　　自己参照(自分の出力を自分の入力として再入力する)と類似していると考えられる。  
  →以上、動作(選択性/自己参照)は類似しているが、機構は全く別、である。
* 神経細胞単体、LLMでいう所のパラメタについてみると、後に参照する、金子氏の脳神経学的観察によると、  
  ・前頭前野では、A10神経系(ドーパミン駆動)に代表される、アミン系神経系が稼動している。  
  ・アミン系神経系には、興奮性と抑制系の2種類の細胞がある。  
  ・神経素子は、シグモイド関数を用いるアナログ非線型素子で、デジタルに近似すると、On/Offの2値である。  
  ・物理的空間上での配置の制限により、一つの神経素子は、近傍の限られた神経素子とだけ結合している。
* 以上のように、生物のニューラルは、近傍の局所間結合のみで、結合強度も、興奮系と抑制系の2種類の  
  On/Offの2値しかない。これが意味する所は、このように制限された結合の数と値でも、自己注意機構は実現できる、  
  言い換えれば、現在のLLMが、非常に無駄に、計算機リソースを消費して、この動作をシミュレートしている、という事が示されている。

### \[2章:LLMの実装改善案の検討]

#### (2-1:1.58bit(BitNet系)とは？)

* 処理が重すぎる現在のTransformerの改善策として、パラメタ値として、 \[1, 0, -1]の３つの離散値のみにする  
  1.58bit(≒log₂3)(BitNet系)と呼ばれる構成法が提案されている(2024,The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits)
* 生物のニューロンは、先に示した通り、興奮性(1)と抑制性(-1)の2種の結合が、On(1)かOff(0)かの状態になるので、結合を上記の3値で表すのは、極めて妥当な割り当てと言える。
* 演算的にも、使用メモリ量が激減し、高コスト演算である乗算を省略して加減算のみで行列演算できるなど、メリットが多い。
* かつ、上記の論文によれば、LLMのような大規模なモデルになると、旧来の実数ベースのLLMと性能的に遜色ないとされており、めぼしいデメリットが見当たらない状況である。

→1.58bit(BitNet系)は、現在のTransformer実装を大幅に改善する可能性を秘めており、今後の、更なる研究の進展が期待される。

#### (2-2:1.58bit(BitNet系)の課題)

* (現在の)1.58bit(BitNet系)は、メモリと演算コストを大幅に改善したが、基本がTransformerである限り、  
  全ニューロン間結合的である、つまり、巨大行列演算によってシミュレートしなければならない、という
  Transformerの根本的コストは軽減出来ない。
* 真に、生物実装的な実装コスト低減を図る為には、ニューロン間が局所的結合(疎結合)である構成での実装が必要である。
* しかし、ニューロン間を疎結合すると、Transformerの学習則である、BP(誤差逆伝搬)法や各種レシピが使えなくなるので、  
  疎結合ニューラルに即した、BP法に代わる学習則が必要となる。  
  また、疎結合自体も、具体的に、どのニューラルに接続するか？という結合の仕方の方法論も検討する必要がある。  
  →これらは、現時点では、いずれも未解決である。

### \[3章:ED(誤差拡散)法について]

#### (3.1　ED法の出発点：BPへの違和感と「教師信号の伝達仮説」)
金子氏の問題意識は明快である。  
多層ネットワークの教師あり学習としてBP（誤差逆伝播）は実用的だが、生体の学習理論として見ると   
「誤差が軸索を逆流しながら演算される」点が不自然であり、そこで別の伝達機構を仮定する。

金子氏はこの「誤差（教師）信号」の伝達を、細胞外空間を介した化学物質（アミン系神経伝達物質）の  
拡散・ブロードキャストとして捉える。  
すなわち、教師信号は結線（シナプス）に沿って一対一で戻るのではなく、近傍の神経群（コラム）が  
同じ“共有信号”を受け取るという仮説である。

　要点：誤差を「逆向きの計算」ではなく「空間的ブロードキャスト」と見なす。  
　その結果、層ごとの誤差逆伝播（連鎖律）が不要になる。

#### (3.2　ED法の第二の柱：興奮性／抑制性を“細胞タイプ”として固定する)
ED法の本質は「誤差を拡散させる」だけではない。  
金子氏が強調するもう一つの要点は、神経細胞が興奮性・抑制性の二タイプに分かれ、細胞タイプが決まると、  
その出力は常に同種の効果を持つ（興奮性なら常に興奮的、抑制性なら常に抑制的）という事実である。

従来の人工ニューラルネットでは、興奮／抑制の符号は主に「シナプス（重み）の符号」として扱われ、  
細胞タイプとして固定する設計は中心に置かれない。  
ED法ではここを前提にすることで、「出力を上げたい／下げたい」時に、どの方向に重みを動かすべきかが  
ローカルに一意に決まるように設計される。

　要点：細胞タイプ（E/I）を導入し、更新方向をローカルに決められるようにする。  
　これにより“勾配”を計算せずとも「下る方向」を構成で保証する。

#### (3.3　ED法のモデル像：コラム共有信号 × ローカル更新)
金子氏の原典を、機構として一言にまとめるならこうなる。
* グローバル信号：出力層の誤差に由来する共有信号（アミン濃度）が、各層・各コラムへ拡散する
* ローカル判断：各結合（シナプス）は、自分の局所情報（入力・出力・細胞タイプ）と共有信号だけで、  

重み更新方向を決める

これは、BPのように「出力層で生まれた誤差を、連鎖律で層ごとに変換して戻す」発想と対照的である。  

　要点：“全体誤差×局所可塑性”。  
　誤差の拡散はグローバル、学習則の適用はローカル。

#### (3.4　学習則の骨格：d⁺/d⁻（符号別誤差信号）と sign 更新)
原典では、出力層の誤差を「符号別の濃度」に分解する。  
* 出力を上げたい（教師 > 出力）なら d⁺ が立つ（d⁻=0）
* 出力を下げたい（教師 < 出力）なら d⁻ が立つ（d⁺=0）

そして、拡散により 全層で同じ d⁺/d⁻ を共有する（層によって変形しない）。

重み更新は、興奮性結合用と抑制性結合用で式が分かれるが、機能的には次の一点に集約できる：

* 更新量は「誤差の大きさ × 局所の符号（sign）」で決まり、  
方向（−1,0,+1）を中心にした更新になる

金子氏はこの性質を明確に言い切っている。  
　「このED法では必ずウェイトの絶対値が増える方向に学習が進行します」  
この一文は、ED法の強みと弱みの両方を端的に表す。

* 強み：方向がぶれず、学習が安定しやすい／速く進む
* 弱み：ゲインが増え続け、飽和・硬直・暴走の危険を内包する

　要点：EDは“勾配の精密さ”ではなく“方向の確実さ”で勝つ。  
　その代償として、ゲイン制御（正規化・抑制）が重要課題になる。

#### (3.5　符号制約：同種は興奮、異種は抑制（設計上の縛り）)
ED法は「細胞タイプ」を導入する代わりに、重みに制約を課す。
* 同種細胞間の結合は興奮性（符号が一方向に縛られる）
* 異種細胞間の結合は抑制性（符号が一方向に縛られる）

この制約によって「出力を上げたいならこの方向」という更新が成立するが、同時に、表現力・汎化に影響し得る。

さらに入力ユニットについても、興奮性入力と抑制性入力を対にして扱う（入力1つがE/Iペアに対応する）。  
これは「E/Iで一意な更新方向」を貫くための設計上の整合である。

　要点：ED法は“自由度”を減らし、代わりに“学習方向の確実性”を得ている。  
　ここがBPとの根本的なトレードオフ。

#### (3.6　実験結果（原典の主張）の要約：速いが、汎化には条件が要る)
原典の実験は、ED法の性格をかなり率直に示す。
##### (1) XOR／パリティ（論理）では、中間ユニットを増やすほど収束が速い
* 中間が十分大きいと数ステップで収束する
* BP（慣性項つき）では、中間を増やすと逆に不収束になる領域がある
* EDはパラメータに鈍感で、同条件で多くの問題が回る、という主張

ここから読み取れるのは、ED法が  
　「薄い共有信号 × 大規模並列（中間の多さ）」で勝つ学習則  
であるという点である。

##### (2) 手書き文字（汎化）では、BPより認識率が落ちるが、中間増加で改善する
* EDはノイズを拾いやすい（局所最小に落ちやすい）可能性を示唆
* ただし中間を増やすと認識率が上がり続ける（原典は“無限大でBP同等に近づくのでは”と予想）

ここから読み取れるのは、ED法が
* 分化（表現の役割分担）
* 局在化（どの結合が学習すべきかの絞り込み）
* ゲイン制御（絶対値増大の抑制）

を備えないまま拡張すると、汎化で不利になりうる、という点である。

　要点：EDは「容量（中間）で殴る」設計と相性が良い一方、  
　汎化には分化・局在化・ゲイン制御が重要になる。

#### (3.7　ED法の射程（原典の推測）：前頭前野・報酬系との接続)
原典の終盤で金子氏は、ED法が脳内で使われるならどこか、という推測を述べる。  
アミン系（ドーパミン等）と学習・報酬が関わる部位として、前頭前野を候補に挙げ、  
行動学習（強化）との親和性を示唆している。

ここは「確証」ではなく「仮説」だが、ED法を
* 画像認識などの“知覚”より
* 行動選択などの“意思決定（前頭前野的機能）”

に向けた学習則として位置づける視点は、本稿で扱う  
**「Transformer以後の軽量“前頭前野”実装」**という問題意識と直結する。

#### (3.8　小結：原典から得られる、現代向けの読解ポイント)
原典を、現代の「次世代LLM × 低bit × 別学習則」という文脈に置き直すと、次が要約となる。
1. ED法は、BPの連鎖律を捨て、共有誤差信号（ブロードキャスト）＋局所更新で教師あり学習を成立させようとする
2. 細胞タイプ（E/I）と符号制約により、更新方向をローカルに一意化することで、勾配計算なしに「下る方向」を作る
3. 更新が sign 中心で、低bit・方向量子化の思想と親和する
4. 一方で、重み絶対値が増え続ける性質を持ち、ゲイン制御が必須課題となる
5. 実験は「中間を増やすほど速くなる」性格を示し、容量（並列性）で勝つ設計思想が見える
6. 汎化では課題が出やすく、分化・局在化・ゲイン制御が揃わないと性能と安定性で不利になり得る

### \[4章:ED法を、1.58bit(BitNet系)の学習則とする際の課題と将来展望]
#### (ED法の成立条件と病理)
――構造的未完成性から見える設計課題

Transformer以後の深層学習は、誤差逆伝播法（BP法）を中核としながらも、  
正規化・残差・適応最適化といった多数の補助構造を付加することで  
ようやく大規模化・安定化を達成した。

一方、ED法（誤差拡散法）は、BPとは異なる思想に基づき、  
誤差を空間的に拡散させることで多層学習を成立させようとした手法である。  
しかしその設計は未完成のまま放棄され、  
結果として「奇妙に速く収束するが、スケールしない手法」として  
歴史の周縁で忘れ去られた。

本章では、ED法の問題点を単なる欠陥としてではなく、  
成立条件が未実装であった結果として現れた病理として再整理する。  
そのために、当時のED実装・当時のBP実装・現代BPの解法を比較し、  
今後ED法を再検討する際に必要となる設計要件を明確化する。

#### (4.1 局在化の欠如がもたらす病理)
――誤差混線と「てんかん様暴走」

ED法は、誤差情報（d⁺ / d⁻）を神経細胞外の化学物質拡散に擬して  
空間的に共有される信号として扱う。  
これは、BPにおける「誤差が軸索を逆流する」という  
生物学的に不自然な仮定を避けるための設計である。

しかし、この設計は同時に、  
誤差の局在性が失われた場合の暴走リスクを孕む。  
誤差が本来影響すべきでない結合群にまで及ぶと、  
ネットワーク全体が同期的に反応し、  
入力に対する選択性を失う。

これは生理学的には、  
局所皮質に限定されるべき活動が全域に波及する  
てんかん様の病理に対応する。

##### 当時のED実装
興味深いことに、当時のED実装では、  
この問題は理論的工夫ではなく、実装上の割り切りによって回避されている。

具体的には、
* 複数出力（multi-output）を  
完全に独立した結合集合として分離
* 各出力に対応する誤差信号は  
他の出力に一切影響しない構造を取る

つまり、  
「誤差は拡散するが、混ざる余地がそもそも存在しない」  
という、最も素朴な局在化である。

この方法は小規模問題では有効だが、  
語彙数が数万〜数十万に及ぶLLMでは成立しない。

##### 当時のBP実装
当時のBP実装では、誤差は明示的な計算経路を逆流するため、  
誤差混線という概念自体が存在しない。  
局在性は構造ではなく、計算手順によって保証されている。

##### 現代BPでの解
現代のTransformer系では、この問題は結果的に解消されている。  
* multi-head attention による意味空間の分割
* 出力層での語彙単位の誤差分離
* 厳密な計算グラフ管理

重要なのは、  
局在化が設計目標として明示されていないにもかかわらず、  
スケールの過程で必然的に実現された点である。

##### 今後のED法で必要な方針
ED法を再検討する場合、  
誤差拡散を単純な全空間ブロードキャストとして扱うことはできない。

必要なのは、
* コラム／クラスタ単位で制御された誤差共有
* 語彙・意味クラスタ単位での誤差分離
* 拡散の範囲と影響度を選択する機構

すなわち、
**「誤差を拡散させるか否か」ではなく
「誤差の拡散幾何をどう設計するか」**が核心となる。

#### (4.2 ゲイン制御の欠如がもたらす病理)
――固着・無気力・暴走

ED法の学習則は、原理的に  
重みの絶対値が増加する方向にのみ更新が進む。  
これは高速な誤差低下をもたらす一方で、  
長期学習において以下の病理を引き起こす。

* 出力が固定化される固着
* 全入力に対して鈍感になる無気力
* 特定入力への過剰反応（暴走）

##### 当時のED実装
ソースコード上には res という  
重み減衰を示唆する変数が存在するが、  
実際には使用されていない。

これは、作者自身が  
ゲイン制御の必要性に気づいていた可能性を示すが、  
実装に至らなかったことを意味する。

##### 当時のBP実装
当時のBPも、慣性項（momentum）以外の  
ゲイン制御機構を持たない。  
正規化や減衰は存在せず、  
スケールすると同様に破綻する。

##### 現代BPでの解
現代BPでは、
* LayerNorm / RMSNorm
* weight decay
* gradient clipping
* Adam系最適化

といった複数の仕組みが重ねられている。  
重要なのは、  
ゲイン制御が学習則の外部に後付けされている点である。

##### 今後のED法で必要な方針
ED法では、BPと同一の解を導入する必要はない。

対応物として考えられるのは、
* d⁺ / d⁻ の正規化
* 更新量の大きさ制限（符号は保持）
* 疎結合による反応余地の確保

ED法におけるゲイン制御は、  
誤差を減らすためではなく、  
学習を持続可能にするための条件である。

#### (4.3 構造的冗長性とスケールの錯覚)
ED法では「中間層を増やすほど収束が速くなる」という性質が確認されている。  
これは短期的には魅力的だが、  
そのまま拡張すると計算資源・安定性の両面で破綻する。

この性質は、  
ED法が冗長性を利用して確率的に正解に近づく手法であることを示している。  
すなわち、  
冗長性は目的ではなく、制御されるべき資源である。

#### (第4章まとめ)
ED法の問題点は、  
設計思想の誤りではなく、  
成立条件が未実装のまま放置されたことに起因する。

現代BPが20年かけて獲得した安定化手法は、  
ED法にとっても不可欠であるが、  
同一の形で導入する必要はない。

ED法が再び検討に値する手法となるか否かは、  
本章で整理した成立条件を  
意識的に設計できるかどうかにかかっている。

#### (第4章補助資料)
##### 図4-1：学習構造の対応関係（概念マップ）

|  学習の核   ||||
|:-:|:-:|:-:|:-:|
|| 当時のED法 | 当時のBP法 | 現代BP（Trf）|
| 誤差伝達    | 空間拡散<br>(d⁺/d⁻)    | 逆伝播<br>(勾配)     | 逆伝播<br>(勾配)       |
| 局在化      | 構造で回避<br>(完全分離) | 経路で保証 | 暗黙的に達成<br>(Attention) |
| ゲイン制御  | 未実装<br>(痕跡あり)    | 慣性のみ   | 正規化群<br>(LN等)     |
| スケール性  | 冗長頼み  | 破綻       | 成立         |
| 安定化戦略  | 構造制約  | パラメタ調 | 多層防御     |

##### 表4-1：問題別・実装比較表
① 誤差の局在化（誤差混線問題）
|観点|当時のED|当時のBP|現代BP|
|:-|:-|:-|:-|
|誤差の流れ|空間拡散|経路逆流|経路逆流|
|混線回避|完全分離（構造）|自然に回避|Attentionで分割|
|スケール耐性|×|△|◎|

注記  
EDは「誤差拡散」という危険な設計を、  
完全分離という原始的だが確実な方法で抑え込んでいた。

② ゲイン制御（重み暴走・固着）
|観点|当時のED|当時のBP|現代BP|
|:-|:-|:-|:-|
|減衰|変数のみ（未使用）|なし|weight decay|
|正規化|なし|なし|LN / RMSNorm|
|クリップ|なし|なし|gradient clipping|
|安定性|小規模限定|調整依存|構造的安定|

注記  
EDコードに res が残っている事実は、  
作者が問題に気づいていたが、未到達だった証拠。

③ 冗長性とスケール戦略
| 観点     | 当時のED  | 当時のBP | 現代BP  |
|:-|:-|:-|:-|
| 中間層    | 多いほど速い | 最適値あり | 深層＋残差 |
| 冗長性    | 本質     | 副作用   | 制御対象  |
| スケール戦略 | 数で殴る   | 不可    | 構造で解決 |

##### 図4-2：ED法が止まった地点とBPが進んだ方向（進化図）
```
        ┌──────────┐
        │  ED法    │
        │（1999）  │
        └────┬─────┘
             │
   ┌─────────┴─────────┐
   │                     │
誤差拡散              冗長性活用
（正しい方向）        （高速収束）
   │                     │
   ├───× ゲイン制御未実装
   ├───× 局在化の一般解なし
   └───× スケール戦略不在

             ↓ 時間

        ┌──────────┐
        │  BP法    │
        │（2000s） │
        └────┬─────┘
             │
        正規化・残差・最適化
             │
        ┌────▼─────┐
        │ Transformer│
        └──────────┘
```

### \[巻末付録:ED法の発明者・金子　勇氏の足跡と偉業]
* https://ja.wikipedia.org/wiki/%E9%87%91%E5%AD%90%E5%8B%87\_(%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9E%E3%83%BC)
* Winny事件の当事者として、非技術者にも(悪い意味で)広く知られ、映画まで作られた方であるが、  
  犯罪者というより、むしろ、卓越した先見性を持つ、天才プログラマーだったと評するのが、妥当であろう。  
  Transformerが登場する10年も前の2013年に、43歳という若さで、急逝された事が、実に惜しまれる。
* 彼がAI関連で注目を集める事になったのは、AI搭載の3D格闘ゲーム「Neko Fight」を、今から20年以上も前の昔、  
  1999～2013年頃に開発したからである。  
  https://forest.watch.impress.co.jp/article/2002/12/02/okiniiri.html  
  https://www.youtube.com/watch?v=IdX-1CC-QoU  
  このゲームでは、対戦相手が、自身と相手のダメージ信号を教師として、こちらの動きを模倣しながら、  
  自らの体の動きを体得するという、現代でもあまり見られない、完全AI学習型の驚くべき敵アルゴリズムを持っていた。
* 当時のニューラルネットワークの教材レベルでは、中間層数が10～50が関の山であったにも関わらず、  
  このゲームでは、中間層数1000を実現し、  
  今のTransformerの学習法である、BP法なら、絶対に不可能である  
  　・現代のGPUがない、CPUだけの並列処理であるOpenMPのみで、  
  　・BP法では、必然である筈の、順序依存なしに、完全並行処理で学習が成立させる  
  という離れ業も実現している。  
  彼は、この、BP法に代わる学習法を、ED(誤差拡散)法と名付けた。
* ED法自体は、1999年に発明され、彼は、論文にまとめたが、当時の学術レベルでは、  
  この学習法の、真の意味や価値が見いだされず、rejectされてしまった。
* 以降は、2007年に、Neko Fightに搭載された後、2013年まで細々とした改良は続けられていたようだが、  
  本質的な改善はなく、ED法が更なる進化を遂げるには、2020年代のTransformer時代を待たなければならなかったが、  
  2013年に彼が急逝した為、ED法は、未完成のまま、忘れられた学習則となってしまった。
* ED法の公式1次資料も、殆どが失われてしまったが、  
  Internet Archieveには、僅かに、本人よる1次情報が残されているので、ここに記載しておく。  
  \[本人によるED法解説]  
  https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/ed.htm  
  \[ED法サンプルプログラム（UNIX汎用、tgz、4KB）]  
  https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/ed.tgz  
  \[(比較対象となる)慣性項ありBP法のサンプルプログラム（UNIX汎用、tgz、4KB）]  
  https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/bp.tgz  
  \[Neko Fightの解説とWindows版実行ファイル]  
  https://web.archive.org/web/20160902231155/http://homepage1.nifty.com/kaneko/nfight.htm  
  https://web.archive.org/web/20160915154005/http://homepage1.nifty.com/kaneko/nfight24.zip
* 筆者は、1年前に、1.58bit(BitNet)LLMに興味を持った際に、その調査過程で、彼とED法を知り、  
  現代でも、まだ実現できていない、オーバーテクノロジー的な、これらの遺物に、驚きを禁じ得ず、  
  これを発掘し、後世の研究者の道しるべとすべく、本稿をまとめる事にした。  
  本稿が、後世の有能なAI研究者たちの、氏とED法を知るきっかけと参考になれば、この上ない、喜びである。
